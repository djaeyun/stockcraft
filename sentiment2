import praw
import pandas as pd
pd.set_option('display.max_columns', 500)
import os
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

reddit = praw.Reddit(user_agent='Comment Extraction (by /u/USERNAME)',
                     client_id=os.environ["REDDIT_CLIENT_ID"], client_secret=os.environ["REDDIT_CLIENT_SECRET"],
                     username=os.environ["REDDIT_USERNAME"], password=os.environ["REDDIT_PASSWORD"])

submission = reddit.submission(id='ae67yi')

comment_list = []
for comment in submission.comments.list():
    comment_dict = {}
    comment_dict["author"] = comment.author
    comment_dict["created"] = comment.created
    comment_dict["body"] = comment.body
    # comment_dict["upvote"] = comment.upvote
    # comment_dict["downvote"] = comment.downvote
    comment_list.append(comment_dict) 

df = pd.DataFrame(comment_list)

company_dict = {}
company_dict["google"] = ["google", "goog"]
company_dict["amazon"] = ["amazon","amzn"]
company_dict["apple"] = ["apple","appl"]
company_dict["micron"] = ["micron","mu"]
company_dict["organogenesis holdings"] = ["organogenesis holdings","orgo"]

stop_words = set(stopwords.words("english"))
ps = PorterStemmer()

tokenized_list = []
for comment in submission.comments.list():
    #Tokenize each word in a comment
    words = word_tokenize(comment.body)
    for r in words:
        #Eliminate stop words in list of tokenized words
        if r not in stop_words:
            #Stem words to root in list of tokenized words
            ps.stem(r)
            tokenized_list.append(r)

